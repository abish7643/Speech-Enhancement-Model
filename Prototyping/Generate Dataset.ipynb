{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import soundfile as sf\n",
    "from audio_utilities import FeatureExtraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_filepaths(dataset_dir, filter_format=\"wav\", get_duration=False):\n",
    "\n",
    "    filepath_array = []\n",
    "    speech_duration = 0\n",
    "\n",
    "    for i, (path, dir_name, file_name) in enumerate(os.walk(dataset_dir)):\n",
    "        for file in file_name:\n",
    "            if filter_format in file:\n",
    "                filepath = path + \"/\" + file\n",
    "                filepath_array.append(filepath)\n",
    "\n",
    "                if get_duration:\n",
    "                    load_file = sf.SoundFile(filepath)\n",
    "                    speech_duration += len(load_file) / load_file.samplerate\n",
    "\n",
    "    print(\"Got {} .{} Files\".format(len(filepath_array), filter_format))\n",
    "    if get_duration:\n",
    "        print(\"Total Duration: {}m\".format(int(speech_duration/60)))\n",
    "\n",
    "    return filepath_array\n",
    "\n",
    "def add_noise_speech(speech, noise, snr=5):\n",
    "\n",
    "    rms_speech = sqrt(np.mean(speech ** 2))\n",
    "    rms_noise_req = sqrt(rms_speech ** 2/pow(10, snr/10))\n",
    "\n",
    "    rms_noise = sqrt(np.mean(noise ** 2))\n",
    "    noise_mod = noise * (rms_noise/rms_noise_req)\n",
    "\n",
    "    return speech + noise_mod\n",
    "\n",
    "def get_melbands_gain(clean_speech_stft, noisy_speech_stft, melbands=22):\n",
    "\n",
    "    clean_mel = audio_utils.get_melspectrogram(audio_stft=clean_speech_stft, number_of_melbands=melbands)\n",
    "    noisy_mel = audio_utils.get_melspectrogram(audio_stft=noisy_speech_stft, number_of_melbands=melbands)\n",
    "\n",
    "    gains_speech = np.sqrt(np.divide(clean_mel, noisy_mel))\n",
    "    # gains_speech = np.where(gains_speech <= 0.01, 0.01, gains_speech)\n",
    "    # gains_speech = np.divide(gains_speech, np.max(gains_speech))\n",
    "\n",
    "    return gains_speech\n",
    "\n",
    "def get_features(clean_speech, noisy_speech, melbands=22):\n",
    "\n",
    "    # Extract MFCC & Relative Derivatives\n",
    "    noisy_speech_stft = audio_utils.stft(noisy_speech)\n",
    "    noisy_speech_mfcc = audio_utils.get_mfccs_from_spectrogram(noisy_speech_stft,\n",
    "                                                               number_of_melbands=22)\n",
    "    noisy_speech_mfcc_delta, \\\n",
    "    noisy_speech_mfcc_delta2 = audio_utils.get_mfccs_delta(noisy_speech_mfcc,\n",
    "                                                           number_of_melbands=9)\n",
    "\n",
    "    # Extract Spectral Centroid & Bandwidth\n",
    "    noisy_speech_spec_centroid = audio_utils.get_spectral_centroid(audio_stft=noisy_speech_stft)\n",
    "    noise_speech_spec_bandwidth = audio_utils.get_spectral_bandwidth(audio_stft=noisy_speech_stft)\n",
    "\n",
    "    # Extract Gains\n",
    "    speech_concat_stft = audio_utils.stft(clean_speech)\n",
    "    speech_melband_gains = get_melbands_gain(clean_speech_stft=speech_concat_stft,\n",
    "                                             noisy_speech_stft=noisy_speech_stft,\n",
    "                                             melbands=melbands)\n",
    "\n",
    "    return  noisy_speech_mfcc, noisy_speech_mfcc_delta, noisy_speech_mfcc_delta2, \\\n",
    "            noisy_speech_spec_centroid, noise_speech_spec_bandwidth, speech_melband_gains\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 4 .wav Files\n",
      "Total Duration: 4m\n",
      "Got 136 .wav Files\n",
      "Total Duration: 6m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_rate = 16000\n",
    "frame_length = 1024\n",
    "hop_length = 512\n",
    "window_length = 1024\n",
    "window_function = \"vorbis\"\n",
    "number_of_melbands = 22\n",
    "number_of_features = 42\n",
    "\n",
    "snr_req = [-5, 0, 5]\n",
    "# features_speech = []\n",
    "# features_gain = []\n",
    "features_speech = np.ndarray((number_of_features,0))\n",
    "features_gain = np.ndarray((number_of_melbands, 0))\n",
    "\n",
    "audio_utils = FeatureExtraction(sampling_rate=sampling_rate,\n",
    "                                frame_length=frame_length, hop_length=hop_length,\n",
    "                                window_length=window_length, window_function=window_function)\n",
    "\n",
    "noise_database_path = \"Dataset Structure/Dataset/Noise\"\n",
    "speech_database_path = \"Dataset Structure/Dataset/Speech\"\n",
    "\n",
    "noise_file_paths = get_filepaths(dataset_dir=noise_database_path, get_duration=True)\n",
    "speech_file_paths = get_filepaths(dataset_dir=speech_database_path, get_duration=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for snr in snr_req:\n",
    "    print(\"\\n========== SNR {} ============\".format(snr))\n",
    "\n",
    "    for noise_file_path in noise_file_paths:\n",
    "\n",
    "        # Load Noise\n",
    "        noise_file = audio_utils.load_audiofile(noise_file_path)\n",
    "        print(\"\\nCurrently Used Noise:\", noise_file_path, len(noise_file))\n",
    "\n",
    "        speech_file_iterator = 0\n",
    "        while speech_file_iterator < len(speech_file_paths):\n",
    "\n",
    "            speech_file = audio_utils.load_audiofile(speech_file_paths[speech_file_iterator])\n",
    "            speech_concat = speech_file\n",
    "\n",
    "            # Concat Speech Till Size of Noise\n",
    "            while len(speech_concat) < len(noise_file):\n",
    "\n",
    "                speech_file_iterator += 1\n",
    "\n",
    "                # Break when file ends\n",
    "                if speech_file_iterator >= len(speech_file_paths):\n",
    "                    # print(speech_file_iterator)\n",
    "                    break\n",
    "                else:\n",
    "                    speech_file = audio_utils.load_audiofile(speech_file_paths[speech_file_iterator])\n",
    "                    speech_concat = np.concatenate((speech_concat, speech_file))\n",
    "                    print(\"Audio To Be Added: \", speech_file_paths[speech_file_iterator])\n",
    "\n",
    "                    if len(speech_concat) >= len(noise_file):\n",
    "\n",
    "                        # Truncate Speech Array to Noise Length\n",
    "                        speech_concat = speech_concat[:len(noise_file)]\n",
    "\n",
    "                        # Add Noise to Speech\n",
    "                        noisy_speech = add_noise_speech(speech_concat, noise_file, snr=snr)\n",
    "\n",
    "                        # Get Features\n",
    "                        mfcc, mfcc_d, mfcc_d2, \\\n",
    "                        spec_centroid, spec_bandwidth, gains = get_features(clean_speech=speech_concat,\n",
    "                                                                            noisy_speech=noisy_speech)\n",
    "\n",
    "                        # print(len(mfcc), len(mfcc_d), len(mfcc_d2),\n",
    "                        #       len(spec_centroid), len(spec_bandwidth), len(gains))\n",
    "\n",
    "                        # Add Features to Array\n",
    "                        features = np.concatenate((mfcc, mfcc_d, mfcc_d2,\n",
    "                                                   spec_bandwidth, spec_centroid), axis=0)\n",
    "\n",
    "                        features_speech = np.concatenate((features_speech, features), axis=1)\n",
    "                        features_gain = np.concatenate((features_gain, gains), axis=1)\n",
    "\n",
    "                        print(\"Added Noise to Speech: \", features_speech.shape, features_gain.shape, \"\\n\")\n",
    "\n",
    "                        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save to File"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Saving To File\")\n",
    "np.savez_compressed(\"feature_dataset.npz\", speech_features=features_speech, gains=features_gain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading From File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading From File\n",
      "(152154, 42) (152154, 22)\n",
      "(152000, 42) (152000, 22)\n",
      "(76, 2000, 42) (76, 2000, 22)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Loading From File\")\n",
    "\n",
    "filename = \"feature_dataset.npz\"\n",
    "\n",
    "with np.load(filename) as data:\n",
    "    speech_features = data[\"speech_features\"]\n",
    "    gains = data[\"gains\"]\n",
    "    # print(np.max(gains))\n",
    "\n",
    "    # gains = np.clip(gains, 0, 1)\n",
    "    # print(np.max(gains))\n",
    "\n",
    "    # Reshape (if Reqd)\n",
    "    speech_features = speech_features.transpose()\n",
    "    gains = gains.transpose()\n",
    "\n",
    "    print(speech_features.shape, gains.shape)\n",
    "\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(speech_features, gains,\n",
    "    #                                                     test_size=0.3)\n",
    "\n",
    "    # x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train,\n",
    "    #                                                                 test_size=0.2)\n",
    "\n",
    "    window_size = 2000\n",
    "    number_of_sequences = int(len(speech_features)/window_size)\n",
    "\n",
    "    x = speech_features[:number_of_sequences*window_size]\n",
    "    y = gains[:number_of_sequences*window_size]\n",
    "\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    x_train = np.reshape(x, (number_of_sequences, window_size, x.shape[1]))\n",
    "    y_train = np.reshape(y, (number_of_sequences, window_size, y.shape[1]))\n",
    "\n",
    "    print(x_train.shape, y_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "from keras.constraints import min_max_norm\n",
    "import h5py\n",
    "\n",
    "from keras.constraints import Constraint\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.42\n",
    "#set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "def my_crossentropy(y_true, y_pred):\n",
    "    return K.mean(2*K.abs(y_true-0.5) * K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "\n",
    "def mymask(y_true):\n",
    "    return K.minimum(y_true+1., 1.)\n",
    "\n",
    "def msse(y_true, y_pred):\n",
    "    return K.mean(mymask(y_true) * K.square(K.sqrt(y_pred) - K.sqrt(y_true)), axis=-1)\n",
    "\n",
    "def mycost(y_true, y_pred):\n",
    "    return K.mean(mymask(y_true) * (10*K.square(K.square(K.sqrt(y_pred) - K.sqrt(y_true))) + K.square(K.sqrt(y_pred) - K.sqrt(y_true)) + 0.01*K.binary_crossentropy(y_pred, y_true)), axis=-1)\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    return K.mean(2*K.abs(y_true-0.5) * K.equal(y_true, K.round(y_pred)), axis=-1)\n",
    "\n",
    "class WeightClip(Constraint):\n",
    "    '''Clips the weights incident to each hidden unit to be inside a range\n",
    "    '''\n",
    "    def __init__(self, c=2):\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, p):\n",
    "        return K.clip(p, -self.c, self.c)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "            'c': self.c}\n",
    "\n",
    "reg = 0.000001\n",
    "constraint = WeightClip(0.499)\n",
    "\n",
    "print('Build model...')\n",
    "main_input = Input(shape=(None, 42), name='main_input')\n",
    "tmp = Dense(24, activation='tanh', name='input_dense', kernel_constraint=constraint, bias_constraint=constraint)(main_input)\n",
    "vad_gru = GRU(24, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='vad_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(tmp)\n",
    "vad_output = Dense(1, activation='sigmoid', name='vad_output', kernel_constraint=constraint, bias_constraint=constraint)(vad_gru)\n",
    "noise_input = keras.layers.concatenate([tmp, vad_gru, main_input])\n",
    "noise_gru = GRU(48, activation='relu', recurrent_activation='sigmoid', return_sequences=True, name='noise_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(noise_input)\n",
    "denoise_input = keras.layers.concatenate([vad_gru, noise_gru, main_input])\n",
    "\n",
    "denoise_gru = GRU(96, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='denoise_gru', kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg), kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(denoise_input)\n",
    "\n",
    "denoise_output = Dense(22, activation='sigmoid', name='denoise_output', kernel_constraint=constraint, bias_constraint=constraint)(denoise_gru)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[denoise_output, vad_output])\n",
    "\n",
    "model.compile(loss=[mycost, my_crossentropy],\n",
    "              metrics=[msse],\n",
    "              optimizer='adam', loss_weights=[10, 0.5])\n",
    "\n",
    "\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model\n",
      "Epoch 1/120\n",
      "2/2 [==============================] - 8s 4s/step - loss: 6.2561 - denoise_output_loss: 0.5387 - vad_output_loss: 1.7371 - denoise_output_msse: 0.1459 - vad_output_msse: 0.1531 - val_loss: 6.3357 - val_denoise_output_loss: 0.5881 - val_vad_output_loss: 0.9091 - val_denoise_output_msse: 0.1590 - val_vad_output_msse: 0.0983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/120\n",
      "2/2 [==============================] - 8s 4s/step - loss: 5.7122 - denoise_output_loss: 0.4854 - vad_output_loss: 1.7155 - denoise_output_msse: 0.1372 - vad_output_msse: 0.1572 - val_loss: 5.5319 - val_denoise_output_loss: 0.5102 - val_vad_output_loss: 0.8589 - val_denoise_output_msse: 0.1455 - val_vad_output_msse: 0.0960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/120\n",
      "2/2 [==============================] - 7s 4s/step - loss: 5.1865 - denoise_output_loss: 0.4337 - vad_output_loss: 1.6973 - denoise_output_msse: 0.1284 - vad_output_msse: 0.1617 - val_loss: 4.9512 - val_denoise_output_loss: 0.4541 - val_vad_output_loss: 0.8189 - val_denoise_output_msse: 0.1357 - val_vad_output_msse: 0.0950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/120\n",
      "Training Interrupted\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Model\")\n",
    "try:\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size, epochs=120, validation_split=0.2)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training Interrupted\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}